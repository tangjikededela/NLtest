{% macro CI(R) %}{{Accuracy}} CI{% endmacro -%}
{% macro CI(R) %}{{Precision}} CI{% endmacro -%}
{% macro CI(R) %}{{AUC}} CI{% endmacro -%}

The model you choose is '{{ model }}' which belong to 'scikit-learn' package,
If you try to make a classification  model that determine if {{ycol}}
by changing {{Xcol}},
this will be a good choice,
because 'scikit-learn' is designed for machine-learning.
The coefficient(s) is {{cof}},
which means {{CF}}
There is a heatmap shows,
You can check if the classification is accurate or not.
Well, you got a classification rate of {{Accuracy}}.
{% if Accuracy < 0.8 %}
it shows this is not a highly reliable model.
Maybe you could try to change another model.
{% elif Accuracy < 0.9 and Accuracy > 0.8%}
it shows this is a highly reliable model.
{% else %}
it shows this is a very highly reliable model.
{% endif %}

The precision is {{Precision}},
which means when your Logistic Regression model predicted {{ycol}} ,
that patients have {{Precision}} of the time.

And you can see the Receiver Operating Characteristic(ROC) curve
that is a plot of the true positive rate against the false positive rate.
It shows the tradeoff between sensitivity and specificity.
The AUC score 1 represents perfect classifier,
and 0.5 represents a worthless classifier.
AUC score for the case is {{AUC}}.
{% if AUC < 0.7 %}
it shows this is not a highly reliable classifier.
{% elif AUC < 0.8 and AUC > 0.7%}
it shows this may is a useable classifier.
{% elif AUC < 0.9 and AUC > 0.8%}
it shows this is a highly reliable classifier.
{% else %}
it shows this is a almost perfect classifier.
{% endif %}
